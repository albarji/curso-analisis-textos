{"cells": [{"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["# Pr\u00e1ctica: detector de idioma"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["<img src=\"img/multilingual.png\" style=\"width:400px;\">"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["En esta pr\u00e1ctica vamos a construir un detector autom\u00e1tico de idioma, capaz de discriminar texto de 20 idiomas diferentes. Para ello vamos a utilizar \u00fanicamente m\u00e9todo basados en an\u00e1lisis de caracteres, que sin embargo resultan ser muy efectivos para este problema."]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["## Instrucciones"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["A lo largo de este cuaderno encontrar\u00e1s celdas vac\u00edas que tendr\u00e1s que rellenar con tu propio c\u00f3digo. Sigue las instrucciones del cuaderno y presta especial atenci\u00f3n a los siguientes iconos:\n", "\n", "<table>\n", "<tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Deber\u00e1s responder a la pregunta indicada con el c\u00f3digo o contestaci\u00f3n que escribas en la celda inferior.</td></tr>\n", " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Esto es una pista u observaci\u00f3n que te puede ayudar a resolver la pr\u00e1ctica.</td></tr>\n", " <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Este es un ejercicio avanzado y voluntario que puedes realizar si quieres profundar m\u00e1s sobre el tema. Te animamos a intentarlo para aprender m\u00e1s \u00a1\u00c1nimo!</td></tr>\n", "</table>\n", "\n", "Para evitar problemas de compatibilidad y de paquetes no instalados, se recomienda ejecutar este notebook bajo uno de los [entornos recomendados de Text Mining](https://github.com/albarji/teaching-environments/tree/master/textmining).\n", "\n", "Adicionalmente si necesitas consultar la ayuda de cualquier funci\u00f3n python puedes colocar el cursor de escritura sobre el nombre de la misma y pulsar May\u00fasculas+Shift para que aparezca un recuadro con sus detalles. Ten en cuenta que esto \u00fanicamente funciona en las celdas de c\u00f3digo.\n", "\n", "\u00a1Adelante!"]}, {"cell_type": "markdown", "metadata": {"nbgrader": {"grade": false, "locked": false, "solution": false}}, "source": ["## Carga y preparaci\u00f3n de datos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para este ejercicio usaremos el corpus de muestras de frases de diferentes idiomas que puede obtenerse de Tatoeba: https://tatoeba.org/eng/downloads ."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "    Descarga el fichero <i>sentences.tar.bz2</i> de la web de Tatoeba y descompr\u00edmelo. Crea una variable DATAFILE con la ruta completa al fichero descomprimido.\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Al tratarse de un fichero en formato TSV (tab separated file) podemos cargarlo como un Dataframe de pandas con facilidad. Si lo has descargado y has indicado la ruta correctamente, la siguiente celda deber\u00eda cargar los datos y mostrar una porci\u00f3n de los mismos."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "df = pd.read_csv(DATAFILE, sep=\"\\t\", index_col=0, names=[\"lang\", \"text\"])\n", "df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Como podemos ver, cada registro contiene una frase (columna \"text\") y un indicador del idioma al que pertenece (columna \"lang\"). El indicador de idioma sigue el est\u00e1ndar [ISO 639-3](https://en.wikipedia.org/wiki/List_of_ISO_639-3_codes)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Antes de ponernos a trabajar con los datos debemos limpiarlos un poco. La siguiente comprobaci\u00f3n nos demuestra que en el indicador de idioma existen valores desconocidos:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.isnull().sum()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Podemos eliminar esos registros inv\u00e1lidos con la siguiente instrucci\u00f3n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.dropna()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora vamos a comprobar qu\u00e9 idiomas hay presentes en los datos. Para ello vamos a utilizar Counter, una estructura que funciona de manera an\u00e1loga a un diccionario de python, pero que permite llevar la cuenta del n\u00famero de veces que ha aparecido un elemento."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from collections import Counter\n", "\n", "langcounter = Counter(df[\"lang\"])\n", "langcounter"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00a1Son 331 idiomas! Para centrar este ejercicio vamos a focalizarnos en los 20 idiomas m\u00e1s representativos. Podemos obtener 20 los elementos m\u00e1s frecuentes de un Counter de la siguiente manera:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["langcounter.most_common(20)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Esto nos devuelve los 20 elementos m\u00e1s frecuentes, junto con sus frecuencias de aparici\u00f3n. Entre los idiomas m\u00e1s frecuentes encontramos el ingl\u00e9s, italiano, ruso, turco, alem\u00e1n, espa\u00f1ol, hebreo, japon\u00e9s, finlandes, chino mandar\u00edn, ..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "      Crea una variable <b>commonlangs</b> que sea una lista con los nombres de esos 20 idiomas m\u00e1s frecuentes. Tendr\u00e1s que tomar la salida del m\u00e9todo most_common y quedarte solo con los nombres de los idiomas.\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Si todo es correcto, la siguiente l\u00ednea deber\u00eda filtrar los datos para quedarnos solo con las frases de esos 20 idiomas."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df[df[\"lang\"].isin(commonlangs)]\n", "df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora que hemos limpiado los datos, vamos a separar las variables de entrada (texto) de las de salida (idiomas). Para poder introducir las etiquetas de idioma en el modelo tendremos que codificarlas de forma num\u00e9rica: para ello usaremos LabelEncoder de scikit-learn:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n", "\n", "X = df[\"text\"]\n", "labelencoder = LabelEncoder().fit(df[\"lang\"])\n", "y = labelencoder.transform(df[\"lang\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A continuaci\u00f3n vamos a separar los datos en un conjunto para entrenar el modelo y otro para hacer las predicciones:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Con esto tenemos todo listo para construir el modelo."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Modelo de uni-gramas"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para empezar vamos a construir un modelo que simplemente tenga en cuenta el tipo de caracteres que aparecen en el texto para tratar de determinar el idioma. Esto significa que vamos a montar un proceso que convierta un texto dado en un vector de frecuencia de caracteres, para luego poder aplicar un sistema de aprendizaje autom\u00e1tico sobre los vectores que obtengamos. Esta transformaci\u00f3n puede hacerse muy f\u00e1cilmente empleando la clase CountVectorizer de paquete scikit-learn:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import CountVectorizer"]}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["Esta clase nos da funcionalidad para tomar un listado de textos y convertirlos en una representaci\u00f3n num\u00e9rica. Podemos configurar c\u00f3mo va a realizarse esta conversi\u00f3n mediante diferentes par\u00e1metros a la hora de instancia un <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a>:\n", "\n", "* **analyzer**: tipo de elementos del texto que vamos a contar para generar la representaci\u00f3n vectorial\n", "    * *word*: conteo de palabras o n-gramas de palabras\n", "    * *char*: conteo de caracteres o n-gramas de caracteres\n", "    * *char_wb*: conteo de caracters o n-gramas de caracteres dentro de cada palabra\n", "* **ngram_range**: tupla tipo (n, m) que indica que rango de n-gramas vamos a construir. Con (1, 1) tendremos unigramas, mientras que con (1, 3) contaremos desde unigramas hasta trigramas.\n", "* **min_df**: n\u00famero (o fracci\u00f3n) m\u00ednimo de textos en los que tiene que aparecer un elemento para considerarlo en la cuenta. Con esto podemos obviar palabras o caracteres que aparezcan muy poco y por tanto no sean relevantes.\n", "* **max_df**: n\u00famero (o fracci\u00f3n) m\u00e1ximo de textos en los que tiene que aparecer un elemento para considerarlo en la cuenta. Con esto podemos obviar palabras o caracteres que aparezcan en casi todos los textos, y por tanto no sean discriminativos.\n", "* **binary**: hacer cuentas binarias (True) al estilo bag-of-words o hacer cuentas reales de elementos (False)\n", "* **lowercase**: convertir autom\u00e1ticamente todos los textos a min\u00fasculas (True) o no (False)\n", "\n", "Para el caso que nos ocupa queremos crear un CountVectorizer que analize unigramas de caracteres, lo cual se conseguir\u00eda como"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vectorizadorejemplo = CountVectorizer(analyzer = \"char\", ngram_range = (1,1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Una vez constru\u00eddo podemos convertir una lista de textos a vectores usando el m\u00e9todo **fit_transform**:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ejemplos = [\n", "    \"The cat sat on the mat\",\n", "    \"The dog barked at the cat\",\n", "    \"Dog days\"\n", "]\n", "transformados = vectorizadorejemplo.fit_transform(ejemplos)\n", "transformados"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Por eficiencia los vectores calculados se almacenan como una matriz comprimida. Podemos ver los contenidos de esta matriz de la siguiente forma:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["transformados.toarray()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00bfQu\u00e9 significa esto? Podemos preguntar a nuestro objeto vectorizador qu\u00e9 vocabulario ha construido con los textos que hemos proporcionado:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vectorizadorejemplo.vocabulary_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Esto nos indica a qu\u00e9 entrada de los vectores generados se corresponde cada palabra del vocabulario. Efectivamente, el primer vector de *transformados*, que corresponde a la frase \"The cat sat on the mat\" nos est\u00e1 indicando que el car\u00e1cter \"a\" aparece tres veces  (\u00edndice 1 del vector) o que el car\u00e1cter \"c\" aparece una vez (\u00edndice 3 del vector). \n", "\n", "Podemos obtener una representaci\u00f3n m\u00e1s gr\u00e1fica de la vectorizaci\u00f3n con la siguiente funci\u00f3n auxiliar."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import itertools\n", "\n", "def plot_vectorizer_matrix(vectorizer, texts, title='Vectorizer matrix', cmap=plt.cm.Blues):\n", "    \"\"\"\n", "    Generate a visual representation of the matrix produced by a vectorizer over some texts\n", "    \"\"\"\n", "    np.set_printoptions(precision=2)\n", "    matrix = vectorizer.transform(texts).toarray()\n", "\n", "    plt.imshow(matrix, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    keys = [k for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1])]\n", "    plt.xticks(np.arange(len(vectorizer.vocabulary_)), keys)\n", "    plt.yticks(range(len(texts)), texts)\n", "\n", "    thresh = matrix.max() / 2.\n", "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n", "        if matrix[i, j] > 0:\n", "            plt.text(j, i, format(matrix[i, j], 'd'),\n", "                     horizontalalignment=\"center\",\n", "                     color=\"white\" if matrix[i, j] > thresh else \"black\")\n", "\n", "plot_vectorizer_matrix(vectorizadorejemplo, ejemplos)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Aqu\u00ed vemos claramente el resultado de la vectorizaci\u00f3n: el texto *Dog days* se ha transformado a un vector que nos indica que en el texto original hab\u00eda presentes los siguientes caracteres: 1 espacio, 1 `a`, 2 `d`, 1 `o`, 1 `s`, y 1 `y`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para combinar f\u00e1cilmente este proceso de vectorizaci\u00f3n con un modelo de clasificaci\u00f3n vamos a usar un **Pipeline** de scikit-learn. En ejercicios posteriores veremos m\u00e1s detalles sobre esto; de momento nos basta con saber que un Pipeline define una serie de etapas del modelo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.pipeline import Pipeline\n", "from sklearn.linear_model import SGDClassifier\n", "\n", "model = Pipeline([\n", "    ('vectorizer', CountVectorizer(analyzer = \"char\", ngram_range = (1,1))),\n", "    ('classifier', SGDClassifier(max_iter=1))\n", "    ]\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Aqu\u00ed hemos definido un Pipeline que primero aplica la vectorizaci\u00f3n por frecuencias de caracteres que hemos discutido arriba, y despu\u00e9s la pasa a un modelo de clasificaci\u00f3n tipo SGDClassifier. Este es un modelo tipo SVM lineal cuya implementaci\u00f3n est\u00e1 especializada en trabajar con grandes vol\u00famenes de datos. Dado que contamos con casi 5 millones de frases para entrenar, es suficiente con hacer una sola iteraci\u00f3n sobre los datos de entrenamiento.\n", "\n", "Una vez definido, entrenamos el modelo con los datos de entrenamiento que hab\u00edamos preparado. El entrenamiento deber\u00eda tardar en torno a 2 minutos."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%time\n", "model.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Con el modelo ya entrenado vamos a valorar c\u00f3mo de bien lo hemos hecho en el conjunto de datos de test. Para ello podemos usar el m\u00e9todo score del modelo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.score(X_test, y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Deber\u00edas haber obtenido en torno a un 86%-87% de accuracy. Para hacer un an\u00e1lisis m\u00e1s en profundidad de la calidad de este modelo, vamos a pintar la matriz de confusi\u00f3n por idiomas. Para ello nos apoyaremos en la siguiente funci\u00f3n:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_confusion_matrix(cm, classes,\n", "                          normalize=False,\n", "                          title='Confusion matrix',\n", "                          cmap=plt.cm.Blues):\n", "    \"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "    np.set_printoptions(precision=2)\n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "\n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    plt.colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=45)\n", "    plt.yticks(tick_marks, classes)\n", "\n", "    fmt = '.2f' if normalize else 'd'\n", "    thresh = cm.max() / 2.\n", "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n", "        plt.text(j, i, format(cm[i, j], fmt),\n", "                 horizontalalignment=\"center\",\n", "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n", "\n", "    plt.ylabel('True label')\n", "    plt.xlabel('Predicted label')\n", "    plt.tight_layout()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora generamos las predicciones para el conjunto de test, usamos la funci\u00f3n de scikit-learn que calcula la matriz de confusi\u00f3n, y la pintamos con la funci\u00f3n definida arriba."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\n", "\n", "y_pred = model.predict(X_test)\n", "cnf_matrix = confusion_matrix(y_test, y_pred)\n", "plt.figure(figsize=(12,12))\n", "plot_confusion_matrix(cnf_matrix, classes=labelencoder.classes_, normalize=True, title='Normalized confusion matrix')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La matriz de confusi\u00f3n revela que algunos idiomas son muy f\u00e1cilmente identificables: chino mandar\u00edn, hebreo, japon\u00e9s y ruso. Esto tiene sentido porque emplean un juego de caracteres muy diferente al de otros idiomas. Sin embargo el modelo confunde mucho entre s\u00ed los idiomas que son similares: berebe con cabilio, o espa\u00f1ol con italiano y portugu\u00e9s.\n", "\n", "Podemos hacerlo mejor. Pero para ello tendremos que recurrir a n-gramas de caracteres."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Modelo de bi-gramas"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "      Repite los pasos anteriores para construir un modelo de bi-gramas de caracteres. \u00bfObtienes mejor precisi\u00f3n global con este modelo? \u00bfQu\u00e9 confusiones han desaparecido en la matriz de confusi\u00f3n? \u00bfCu\u00e1les permanecen?\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "      Revisar la explicaci\u00f3n anterior de los par\u00e1metros que acepta CountVectorizer. Solo es necesario cambiar este elemento en la definici\u00f3n del Pipeline para obtener el modelo de bigramas.\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## M\u00e1s all\u00e1 de bi-gramas"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Es posible obtener resultados a\u00fan mejores si se emplean tri-gramas o tetra-gramas. Pero dado el tama\u00f1o del dataset esto puede requerir de un gasto excesivo de memoria. Para evitar esto, ser\u00e1 necesario recurrir a otras estrategias de vectorizaci\u00f3n, como [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "      Intenta mejorar los resultados del modelo bi-gramas utilizando HashingVectorizer y un mayor orden de n-gramas.\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 1}