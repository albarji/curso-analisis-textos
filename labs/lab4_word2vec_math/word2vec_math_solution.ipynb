{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: análisis semántico con word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/word2vec.png\" style=\"width:600px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio vamos a utilizar word2vec para estudiar algunas relaciones semánticas entre palabras. Veremos cómo con esta técnica podemos resolver fácilmente los típicos problemas de encontrar palabras extrañas en un conjunto dado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de este cuaderno encontrarás celdas vacías que tendrás que rellenar con tu propio código. Sigue las instrucciones del cuaderno y presta especial atención a los siguientes iconos:\n",
    "\n",
    "<table>\n",
    "<tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Deberás responder a la pregunta indicada con el código o contestación que escribas en la celda inferior.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Esto es una pista u observación que te puede ayudar a resolver la práctica.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Este es un ejercicio avanzado y voluntario que puedes realizar si quieres profundar más sobre el tema. Te animamos a intentarlo para aprender más ¡Ánimo!</td></tr>\n",
    "</table>\n",
    "\n",
    "Para evitar problemas de compatibilidad y de paquetes no instalados, se recomienda ejecutar este notebook bajo uno de los [entornos recomendados de Text Mining](https://github.com/albarji/teaching-environments/tree/master/textmining).\n",
    "\n",
    "Adicionalmente si necesitas consultar la ayuda de cualquier función python puedes colocar el cursor de escritura sobre el nombre de la misma y pulsar Mayúsculas+Shift para que aparezca un recuadro con sus detalles. Ten en cuenta que esto únicamente funciona en las celdas de código.\n",
    "\n",
    "¡Adelante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar un modelo de word2vec es una tarea muy costosa computacionalmente, que además requiere de corpus de texto muy grandes, del orden de miles de millones de palabras. Afortunadamente existen modelos word2vec pre-entrenados que están disponibles de forma pública y con los que podemos trabajar para hacer nuestros análisis semánticos. Uno de ellos es el modelo [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/), entrenado con 100.000 millones de palabras y que se encuentra disponible para descarga en la siguiente dirección: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "      Descarga manualmente el modelo <b>GoogleNews-vectors-negative300</b> en tu máquina, y a continuación crea una variable <i>modelfile</i> con la ruta del fichero descomprido del modelo.\n",
    "  </td>\n",
    " </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "modelfile = \"D:/Tmp/GoogleNews-vectors-negative300.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud semántica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo pre-entrenado word2vec no es más que un diccionario en el que para cada palabra tenemos el vector que la representa. Podemos cargar en memoria este diccionario utilizando el paquete de análisis de texto **gensim**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\textmining-labs\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(modelfile, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es conveniente aplicar la siguiente operación para guardar en memoria únicamente la información relativa a los vectores. Esto hace que no podamos reentrenar la representación vectorial de las palabras, pero para el objetivo de esta práctica no es necesario tal cosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar ahora la representación vectorial de diferentes palabras, por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00173332, -0.04740431, -0.02289596,  0.0407935 ,  0.04353457,\n",
       "       -0.02934553, -0.02354092, -0.07159019, -0.06514061,  0.01838126,\n",
       "       -0.02499207, -0.12576653,  0.03434394, -0.00026957,  0.04385705,\n",
       "        0.03724624,  0.02402463, -0.01547896,  0.02176728,  0.03111915,\n",
       "        0.06288327,  0.04514696, -0.07803975, -0.03918111,  0.02160605,\n",
       "       -0.01757507, -0.10190314,  0.03031296,  0.06223831, -0.05514379,\n",
       "       -0.05159653, -0.04321209, -0.02724942,  0.07030027, -0.1173821 ,\n",
       "       -0.04353457,  0.03176411,  0.08706914, -0.03128039,  0.06062592,\n",
       "        0.03531137, -0.13737576,  0.08900401, -0.00915032,  0.0580461 ,\n",
       "       -0.03724624, -0.00136046,  0.04804927,  0.05159653,  0.08835905,\n",
       "       -0.00592554,  0.03257031,  0.01749445, -0.01031931, -0.05385388,\n",
       "       -0.01918746, -0.11351236, -0.05707866,  0.03772996, -0.02982924,\n",
       "        0.044502  ,  0.09222879, -0.01644639,  0.04288961,  0.0580461 ,\n",
       "       -0.07320257, -0.0039302 ,  0.04643688, -0.05998096,  0.03918111,\n",
       "        0.05320892,  0.07094523, -0.06997779,  0.04192218, -0.03305402,\n",
       "        0.04514696,  0.04192218,  0.00505888,  0.03450518, -0.00919063,\n",
       "       -0.01991303,  0.00733638,  0.05998096, -0.022251  ,  0.01620453,\n",
       "        0.05095157, -0.08255444,  0.0461144 ,  0.09738845,  0.03515013,\n",
       "        0.11093254, -0.07352505, -0.04127722, -0.01749445,  0.06352822,\n",
       "        0.02257348,  0.0230572 , -0.01717197,  0.04772679,  0.00148138,\n",
       "       -0.00334571, -0.0490167 ,  0.07191266, -0.06449566, -0.12899132,\n",
       "        0.02563703, -0.18961723, -0.02612074, -0.01338285, -0.06191583,\n",
       "        0.08384436,  0.05191901,  0.04127722,  0.04643688,  0.06643053,\n",
       "       -0.0193487 ,  0.05578874, -0.09287375,  0.05675618,  0.06320575,\n",
       "        0.03998731, -0.04966166, -0.00229766, -0.07836223,  0.0461144 ,\n",
       "       -0.00276122, -0.01555958,  0.04837174, -0.01878436,  0.03369898,\n",
       "        0.02789437,  0.06997779, -0.00604647,  0.1663988 , -0.0160433 ,\n",
       "        0.07320257, -0.08448932,  0.01128674,  0.05224149, -0.0111255 ,\n",
       "        0.022251  , -0.0831994 ,  0.0152371 , -0.02402463,  0.02628198,\n",
       "        0.01394719, -0.00042577,  0.06675301, -0.04321209, -0.01991303,\n",
       "        0.01249603,  0.04482448,  0.03692377, -0.08126453, -0.07900719,\n",
       "       -0.07803975, -0.01426966, -0.00959373,  0.07771727, -0.00128991,\n",
       "       -0.01660763,  0.0609484 ,  0.1663988 , -0.00220696, -0.01144798,\n",
       "       -0.02499207,  0.02031613, -0.10383801, -0.01241541,  0.07803975,\n",
       "       -0.04224466,  0.05095157,  0.0980334 ,  0.00894877, -0.0564337 ,\n",
       "       -0.07417001,  0.00079612,  0.0349889 , -0.01096426,  0.00481702,\n",
       "       -0.0712677 ,  0.08158701, -0.02450835, -0.03369898,  0.05578874,\n",
       "       -0.01660763,  0.06868788, -0.01273789, -0.07352505,  0.09093888,\n",
       "       -0.01967118, -0.00580461,  0.00592554,  0.02934553,  0.04256713,\n",
       "        0.06030344,  0.04772679,  0.03434394, -0.04385705, -0.10770775,\n",
       "        0.0012748 , -0.03756872, -0.01668825, -0.04321209,  0.00729607,\n",
       "       -0.04837174,  0.03547261,  0.07610488,  0.05030661,  0.07513744,\n",
       "        0.01596268,  0.02241224, -0.02241224, -0.08513427,  0.037085  ,\n",
       "        0.00119922, -0.03966483, -0.07030027,  0.037085  ,  0.04014855,\n",
       "        0.02257348,  0.01620453,  0.09029392, -0.00099767, -0.0012496 ,\n",
       "        0.00063992,  0.06191583, -0.09609853,  0.02982924,  0.08706914,\n",
       "       -0.08448932, -0.09480862, -0.13479593,  0.03482766,  0.13092619,\n",
       "       -0.02402463, -0.01426966, -0.02096109, -0.00019903, -0.03805244,\n",
       "        0.10383801, -0.07352505,  0.04482448, -0.00648988,  0.05062909,\n",
       "        0.00118914, -0.03966483,  0.02144481,  0.0535314 , -0.05514379,\n",
       "        0.01080302, -0.00115891,  0.06223831,  0.0638507 ,  0.04385705,\n",
       "        0.02241224, -0.1128674 , -0.02757189, -0.09738845, -0.00693328,\n",
       "        0.02934553,  0.10706279, -0.1218968 , -0.02837809, -0.01636577,\n",
       "        0.04353457,  0.01459214,  0.11867201,  0.07094523,  0.0877141 ,\n",
       "       -0.09029392,  0.07771727,  0.03772996,  0.02902305,  0.06191583,\n",
       "       -0.10964262,  0.04998413, -0.0119317 , -0.03934235,  0.09158383,\n",
       "        0.03579509, -0.02402463,  0.07674983,  0.00072558,  0.0786847 ,\n",
       "       -0.08223197, -0.0407935 , -0.05256396, -0.00198526,  0.04804927,\n",
       "       -0.00152169, -0.07449248, -0.12512158, -0.12447662, -0.02724942,\n",
       "       -0.01362471,  0.05546627, -0.01547896,  0.05385388,  0.05095157],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[\"queen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la representación vectorial de una palabra resulta oscura de interpretar, se ha comprobado que sigue una lógica semántica y sintáctica. Esto permite hacer aritmética con estos vectores y obtener resultados que son coherentes con lo que cabría esperar. Por ejemplo, si denotamos como $v(word)$ la representación vectorial de una cierta palabra *word* podemos encontrar casos como: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v(king) - v(man) + v(woman) \\simeq v(queen)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar que eso es cierto utilizando la función **most_similar** del objeto que contiene los embeddings. Esta función recibe dos listas de palabras, a contribuir de forma positiva o negativa a la operación aritmética, y devuelve las palabras cuya representación vectorial sea más cercana al vector resultado de la operación, ordenadas por similitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Funciona! Podemos ver más ejemplos, como los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Putin', 0.6845240592956543),\n",
       " ('President_Vladimir_Putin', 0.6755064725875854),\n",
       " ('Kremlin', 0.6264293193817139),\n",
       " ('Medvedev', 0.6225455403327942),\n",
       " ('Vladimir_Putin', 0.6010303497314453),\n",
       " ('President_George_W.', 0.5910208225250244),\n",
       " ('Prime_Minister_Vladimir_Putin', 0.5779234170913696),\n",
       " ('President_Dmitry_Medvedev', 0.5603682994842529),\n",
       " ('Medevedev', 0.545877993106842),\n",
       " ('George_W', 0.5436867475509644)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['Bush', 'Russia'], negative=['USA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paris', 0.7502285242080688),\n",
       " ('Marseille', 0.603843092918396),\n",
       " ('French', 0.601784348487854),\n",
       " ('Colombes', 0.5965863466262817),\n",
       " ('Hopital_Europeen_Georges_Pompidou', 0.5867530107498169),\n",
       " ('Toulouse', 0.577813446521759),\n",
       " ('Parisian', 0.570327639579773),\n",
       " ('Cergy_Pontoise', 0.568040132522583),\n",
       " ('Marseilles', 0.5587785243988037),\n",
       " ('Strasbourg', 0.5559653043746948)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['Madrid', 'France'], negative=['Spain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adios', 0.6326478719711304),\n",
       " ('goodbyes', 0.6136816740036011),\n",
       " ('farewells', 0.5774319767951965),\n",
       " ('farewell', 0.5727684497833252),\n",
       " ('fond_farewell', 0.5445314645767212),\n",
       " ('hasta_luego', 0.5245710015296936),\n",
       " ('bid_farewell', 0.4987291693687439),\n",
       " ('hello', 0.488644003868103),\n",
       " ('arrivederci', 0.4783087372779846),\n",
       " ('adiós', 0.4729732871055603)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['goodbye', 'spanish'], negative=['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.most_similar(positive=['boat', 'air'], negative=['ocean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('risotto', 0.6650041341781616),\n",
       " ('pasta', 0.6559766530990601),\n",
       " ('gnocchi', 0.6452398300170898),\n",
       " ('pizza_margherita', 0.6371422410011292),\n",
       " ('manicotti', 0.6349635720252991),\n",
       " ('osso_buco', 0.6342277526855469),\n",
       " ('di_pesce', 0.6324365735054016),\n",
       " ('ragu', 0.6315066814422607),\n",
       " ('pesce', 0.6304484605789185),\n",
       " ('minestrone', 0.6299175024032593)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['paella', 'Italy'], negative=['Spain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.most_similar(positive=['Harry_Potter', 'evil'], negative=['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enslave_mankind', 0.5105279684066772),\n",
       " ('Necromongers', 0.5103692412376404),\n",
       " ('Artificial_Intelligence', 0.5079575181007385),\n",
       " ('malevolent', 0.5064362287521362),\n",
       " ('demon_lord', 0.5063003897666931),\n",
       " ('necromancer', 0.504085898399353),\n",
       " ('psionic_powers', 0.503738284111023),\n",
       " ('cybernetic_organisms', 0.4959980845451355),\n",
       " ('alien_beings', 0.49571239948272705),\n",
       " ('Overmind', 0.49549269676208496)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['artificial_intelligence', 'evil'], negative=['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluso pueden capturarse relaciones de morfología entre palabras, como las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brother', 0.7627110481262207),\n",
       " ('younger_brother', 0.6856131553649902),\n",
       " ('cousin', 0.6685014963150024),\n",
       " ('uncle', 0.6580697298049927),\n",
       " ('nephew', 0.6526023149490356),\n",
       " ('father', 0.6411104202270508),\n",
       " ('son', 0.6308268308639526),\n",
       " ('elder_brother', 0.5854185819625854),\n",
       " ('brothers', 0.5706700086593628),\n",
       " ('twin_brother', 0.5622221827507019)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['sister', 'he'], negative=['she'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.5713158845901489),\n",
       " ('pets', 0.5167832970619202),\n",
       " ('kitties', 0.49727511405944824),\n",
       " ('pet', 0.484472393989563),\n",
       " ('feline', 0.48316147923469543),\n",
       " ('stray_cat', 0.4815112054347992),\n",
       " ('felines', 0.48110759258270264),\n",
       " ('kitten', 0.47646141052246094),\n",
       " ('dog', 0.47168344259262085),\n",
       " ('puppy', 0.46635887026786804)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['cat', 'many'], negative=['one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ate', 0.47409069538116455),\n",
       " ('eating', 0.45381370186805725),\n",
       " ('eaten', 0.447462260723114),\n",
       " ('eats', 0.43551111221313477),\n",
       " ('microwave_burrito', 0.40894240140914917),\n",
       " ('eat_buffets', 0.40705668926239014),\n",
       " ('banana_muffin', 0.39758825302124023),\n",
       " ('yo_yo_dieted', 0.3958820700645447),\n",
       " ('Rancho_Bernardo_Escondido', 0.3941839337348938),\n",
       " ('Filet_o_Fish', 0.3925513029098511)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar(positive=['eat', 'past'], negative=['present'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "    ¡Busca tú mismo algún ejemplo interesante!\n",
    "  </td>\n",
    " </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lol', 0.5809476375579834), ('.....', 0.5671467781066895), (':-(', 0.5583669543266296), ('haha', 0.5576198101043701), ('taylor_swift', 0.5430734157562256), ('hahaha', 0.5416510105133057), ('RT_@_@', 0.5390990972518921), ('hahah', 0.5386964678764343), ('HAHAHAHAHA', 0.5321018695831299), ('@_ChrisHarrisNFL', 0.5315563678741455)]\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "print(embeddings.most_similar(positive=[':)', 'sad'], negative=['happy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odd-one out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una utilidad interesante de las distancias semánticas entre palabras es de resolver los típicos pasatiempos en los que se debe identificar la palabra que no encaja dentro de un grupo dado. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [\"Obama\", \"Merkel\", \"Putin\", \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está claro que *truck* es la palabra intrusa en esta lista de presidentes del gobierno. Pero esto es algo que sabemos por nuestro amplio conocimiento del mundo y del lenguaje, y para un programa informático no es nada trivial llegar a esta conclusión. Sin embargo gracias a las representaciones semánticas en forma de vector que nos da word2vec podemos hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "      Implementa una función <b>oddoneout</b> que recibe una lista de palabras y un modelo word2vec, y realice los siguientes pasos:\n",
    "    <ol>\n",
    "     <li> Obtener la representación vectorial de cada palabra recibida. Ignora las palabras para las que el modelo no contemple una representación vectorial.</li>\n",
    "     <li> Calcula el vector medio de todas las palabras recibidas.</li>\n",
    "     <li> Calcula la distancia de ese vector medio al vector representativo de cada palabra.</li>\n",
    "     <li> Devuelve la palabra con mayor distancia.</li>\n",
    "    </ol>\n",
    "  </td>\n",
    " </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "import numpy as np\n",
    "def oddoneout(words, embeddings):\n",
    "    vectors = {word : embeddings[word] for word in words if word in embeddings}\n",
    "    mean = np.mean([vectors[word] for word in vectors], axis=0)\n",
    "    dists = {word : np.linalg.norm(vectors[word] - mean) for word in vectors}\n",
    "    sorteddists = sorted(dists, key = lambda x : dists[x])\n",
    "    return sorteddists[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comprobar ahora con el ejemplo de antes si la implementación ha funcionado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truck'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oddoneout(group, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "    ¿Ha funcionado el ejemplo anterior? ¿Puedes pensar en otros ejemplos en los que el algoritmo también fucione? ¿Hay algún caso en el que falle?\n",
    "     En gensim cualquier objeto de modelo word2vec dispone de la función <b>doesnt_match</b> que realiza la misma función que el algoritmo que has implementado, pero con un cálculo de distancias más adecuado a la representación vectorial. En general esta aproximación debería ser mejor que la que has implementado. ¿Encuentras algún caso en el que sea así?\n",
    "  </td>\n",
    " </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
