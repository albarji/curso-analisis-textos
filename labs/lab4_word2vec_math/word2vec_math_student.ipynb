{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Ejercicio: an\u00e1lisis sem\u00e1ntico con word2vec"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"img/word2vec.png\" style=\"width:600px;height:400px;\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["En este ejercicio vamos a utilizar word2vec para estudiar algunas relaciones sem\u00e1nticas entre palabras. Veremos c\u00f3mo con esta t\u00e9cnica podemos resolver f\u00e1cilmente los t\u00edpicos problemas de encontrar palabras extra\u00f1as en un conjunto dado."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Instrucciones"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A lo largo de este cuaderno encontrar\u00e1s celdas vac\u00edas que tendr\u00e1s que rellenar con tu propio c\u00f3digo. Sigue las instrucciones del cuaderno y presta especial atenci\u00f3n a los siguientes iconos:\n", "\n", "<table>\n", "<tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Deber\u00e1s responder a la pregunta indicada con el c\u00f3digo o contestaci\u00f3n que escribas en la celda inferior.</td></tr>\n", " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Esto es una pista u observaci\u00f3n que te puede ayudar a resolver la pr\u00e1ctica.</td></tr>\n", " <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">Este es un ejercicio avanzado y voluntario que puedes realizar si quieres profundar m\u00e1s sobre el tema. Te animamos a intentarlo para aprender m\u00e1s \u00a1\u00c1nimo!</td></tr>\n", "</table>\n", "\n", "Para evitar problemas de compatibilidad y de paquetes no instalados, se recomienda ejecutar este notebook bajo uno de los [entornos recomendados de Text Mining](https://github.com/albarji/teaching-environments/tree/master/textmining).\n", "\n", "Adicionalmente si necesitas consultar la ayuda de cualquier funci\u00f3n python puedes colocar el cursor de escritura sobre el nombre de la misma y pulsar May\u00fasculas+Shift para que aparezca un recuadro con sus detalles. Ten en cuenta que esto \u00fanicamente funciona en las celdas de c\u00f3digo.\n", "\n", "\u00a1Adelante!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Carga y preparaci\u00f3n de datos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Entrenar un modelo de word2vec es una tarea muy costosa computacionalmente, que adem\u00e1s requiere de corpus de texto muy grandes, del orden de miles de millones de palabras. Afortunadamente existen modelos word2vec pre-entrenados que est\u00e1n disponibles de forma p\u00fablica y con los que podemos trabajar para hacer nuestros an\u00e1lisis sem\u00e1nticos. Uno de ellos es el modelo [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/), entrenado con 100.000 millones de palabras y que se encuentra disponible para descarga en la siguiente direcci\u00f3n: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "      Descarga manualmente el modelo <b>GoogleNews-vectors-negative300</b> en tu m\u00e1quina, y a continuaci\u00f3n crea una variable <i>modelfile</i> con la ruta del fichero descomprido del modelo.\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Similitud sem\u00e1ntica"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Un modelo pre-entrenado word2vec no es m\u00e1s que un diccionario en el que para cada palabra tenemos el vector que la representa. Podemos cargar en memoria este diccionario utilizando el paquete de an\u00e1lisis de texto **gensim**:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gensim\n", "embeddings = gensim.models.KeyedVectors.load_word2vec_format(modelfile, binary=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Es conveniente aplicar la siguiente operaci\u00f3n para guardar en memoria \u00fanicamente la informaci\u00f3n relativa a los vectores. Esto hace que no podamos reentrenar la representaci\u00f3n vectorial de las palabras, pero para el objetivo de esta pr\u00e1ctica no es necesario tal cosa."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.init_sims(replace=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Podemos comprobar ahora la representaci\u00f3n vectorial de diferentes palabras, por ejemplo:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["embeddings[\"queen\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Aunque la representaci\u00f3n vectorial de una palabra resulta oscura de interpretar, se ha comprobado que sigue una l\u00f3gica sem\u00e1ntica y sint\u00e1ctica. Esto permite hacer aritm\u00e9tica con estos vectores y obtener resultados que son coherentes con lo que cabr\u00eda esperar. Por ejemplo, si denotamos como $v(word)$ la representaci\u00f3n vectorial de una cierta palabra *word* podemos encontrar casos como: "]}, {"cell_type": "markdown", "metadata": {}, "source": ["$$v(king) - v(man) + v(woman) \\simeq v(queen)$$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Podemos comprobar que eso es cierto utilizando la funci\u00f3n **most_similar** del objeto que contiene los embeddings. Esta funci\u00f3n recibe dos listas de palabras, a contribuir de forma positiva o negativa a la operaci\u00f3n aritm\u00e9tica, y devuelve las palabras cuya representaci\u00f3n vectorial sea m\u00e1s cercana al vector resultado de la operaci\u00f3n, ordenadas por similitud:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['king', 'woman'], negative=['man'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00a1Funciona! Podemos ver m\u00e1s ejemplos, como los siguientes:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['Bush', 'Russia'], negative=['USA'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['Madrid', 'France'], negative=['Spain'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['goodbye', 'spanish'], negative=['english'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['boat', 'air'], negative=['ocean'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['paella', 'Italy'], negative=['Spain'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['Harry_Potter', 'evil'], negative=['good'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['artificial_intelligence', 'evil'], negative=['good'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Incluso pueden capturarse relaciones de morfolog\u00eda entre palabras, como las siguientes:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['sister', 'he'], negative=['she'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['cat', 'many'], negative=['one'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embeddings.most_similar(positive=['eat', 'past'], negative=['present'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "    \u00a1Busca t\u00fa mismo alg\u00fan ejemplo interesante!\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Odd-one out"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Una utilidad interesante de las distancias sem\u00e1nticas entre palabras es de resolver los t\u00edpicos pasatiempos en los que se debe identificar la palabra que no encaja dentro de un grupo dado. Por ejemplo:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["group = [\"Obama\", \"Merkel\", \"Putin\", \"truck\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Est\u00e1 claro que *truck* es la palabra intrusa en esta lista de presidentes del gobierno. Pero esto es algo que sabemos por nuestro amplio conocimiento del mundo y del lenguaje, y para un programa inform\u00e1tico no es nada trivial llegar a esta conclusi\u00f3n. Sin embargo gracias a las representaciones sem\u00e1nticas en forma de vector que nos da word2vec podemos hacerlo."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "      Implementa una funci\u00f3n <b>oddoneout</b> que recibe una lista de palabras y un modelo word2vec, y realice los siguientes pasos:\n", "    <ol>\n", "     <li> Obtener la representaci\u00f3n vectorial de cada palabra recibida. Ignora las palabras para las que el modelo no contemple una representaci\u00f3n vectorial.</li>\n", "     <li> Calcula el vector medio de todas las palabras recibidas.</li>\n", "     <li> Calcula la distancia de ese vector medio al vector representativo de cada palabra.</li>\n", "     <li> Devuelve la palabra con mayor distancia.</li>\n", "    </ol>\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vamos a comprobar ahora con el ejemplo de antes si la implementaci\u00f3n ha funcionado:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["oddoneout(group, embeddings)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<table>\n", " <tr>\n", "  <td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n", "    \u00bfHa funcionado el ejemplo anterior? \u00bfPuedes pensar en otros ejemplos en los que el algoritmo tambi\u00e9n fucione? \u00bfHay alg\u00fan caso en el que falle?\n", "     En gensim cualquier objeto de modelo word2vec dispone de la funci\u00f3n <b>doesnt_match</b> que realiza la misma funci\u00f3n que el algoritmo que has implementado, pero con un c\u00e1lculo de distancias m\u00e1s adecuado a la representaci\u00f3n vectorial. En general esta aproximaci\u00f3n deber\u00eda ser mejor que la que has implementado. \u00bfEncuentras alg\u00fan caso en el que sea as\u00ed?\n", "  </td>\n", " </tr> \n", "</table>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### INSERT YOUR CODE HERE"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 1}